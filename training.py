# -*- coding: utf-8 -*-
"""training.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/199aVp_Md_eFS6zju9erguaYUds_VWJZa

**Installing Required Libraries...**

Note : Tensorflow 1.14 is installed to support tflearn.contrib class that will be used in this project.
"""

# !pip install tensorflow==1.14
# !pip install numpy
# !pip install tflearn

"""**NLTK - Natural Language Toolkit**

This is a powerful python tool used to Tokenize the sentences and stem the words to match the context.

**TensorFlow**

This a tool which is used to build the neural networks model using DNN and ANN. The model that is used in this project is DNN (Deep Neural Networks) which will be elaborated in the further parts of the code.
"""

#integrating NLP
import nltk
from nltk.stem.lancaster import LancasterStemmer
stemmer = LancasterStemmer()

#tensorflow and deep learning model
import numpy as np
import tflearn
import tensorflow as tf
import random

nltk.download('punkt')

# from google.colab import drive
# drive.mount('/content/drive')

import json

"""**Dataset**

The dataset on which the model is going to be trained is in JSON format. The objects are stored as intents. Each object has three properties:
1. Tags - Specifies the context
2. Patterns - Different combinations of a query
3. Responses - A set of responses from which a bot can select and give the user
"""

with open('intents.json',encoding='utf-8') as json_data:
    intents = json.load(json_data)
print(intents)

"""**Data Preprocessing**

The words are extracted/tokenized and they are stemmed using NLTK Tokenizer and Lancaster Stemmer.
"""

words = list()
classes = list()
documents = list()
ignore_words = ['?','!',',','.']

# loop through each sentence in our intents patterns
for intent in intents['intents']:
    for pattern in intent['patterns']:

        # tokenize each word in the sentence
        w = nltk.word_tokenize(pattern)
        #print("w = ", w)

        # add to our words list
        words.extend(w)
        #print(words)

        # add to documents in our corpus
        documents.append((w, intent['tag']))

        # add to our classes list
        if intent['tag'] not in classes:
            classes.append(intent['tag'])

# stem and lower each word and remove duplicates
words = [stemmer.stem(w.lower()) for w in words if w not in ignore_words]
words = sorted(list(set(words)))

# remove duplicates
classes = sorted(list(set(classes)))

print (len(documents), "documents")
print (len(classes), "classes", classes)
print (len(words), "unique stemmed words", words)

"""**Creation of Training Data**

The training data is processed and stored in NumPy array as a matrix. 

Column 1 consists of "bag of words"

Column 2 consists of the "indices of classes"

**Bag of Words**

It is a single dimensional array consisting of 0s and 1s. A '1' indicates the presence of the corresponding word in the sentence and '0' indicates the absence.

 (The words are not ordered)


"""

# create our training data
training = list()
output = list()

# create an empty array for our output
empty = [0] * len(classes)

# training set, bag of words for each sentence
for doc in documents:

    # initialize our bag of words
    bag = list()

    # list of tokenized words for the pattern
    pattern_words = doc[0]

    # stem each word
    pattern_words = [stemmer.stem(word.lower()) for word in pattern_words]

    # create our bag of words array
    for w in words:
        bag.append(1) if w in pattern_words else bag.append(0)

    # output is a '0' for each tag and '1' for current tag
    row = list(empty)
    row[classes.index(doc[1])] = 1

    training.append([bag, row])

# shuffle our features and turn into np.array
random.shuffle(training)
training = np.array(training)

# create train and test lists
train_x = list(training[:,0])
train_y = list(training[:,1])

#print("x = ",train_x)
#print("y = ",train_y)

"""**Defining Neural Networks Model Architecture**

> Indented block

> Indented block





TensorFlow's TFlearn module is used for creating a neural network model for this project.

The four stages of the model are described below:

1. Input layer - consists of 'N' nodes where 'N' is number of elements in "bag of words"

2. Hidden layers - For this model, considering the dataset, 2 hidden layers are implemented with 8 nodes in each hidden layer.

3. Activation Function - softmax (Other functions (sigmoid) are yet to be tried)

4. Output layer - consists of 'M' nodes where 'M' is number of classes.

"""

# reset underlying graph data
tf.reset_default_graph()

# Building neural network - defining model architecture

#loading input data
net = tflearn.input_data(shape=[None, len(train_x[0])])

#defining neural network layers
hidden_layers = 2
for i in range(hidden_layers):
  net = tflearn.fully_connected(net, 8)

#defining activation function
net = tflearn.fully_connected(net, len(train_y[0]), activation='softmax')

#defining regression
net = tflearn.regression(net)

# Define model and setup tensorboard
model = tflearn.DNN(net, tensorboard_dir='tflearn_logs')

# Start training (apply gradient descent algorithm)
model.fit(train_x, train_y, n_epoch=1000, batch_size=8, show_metric=True)
model.save('model.tflearn')

# !pip uninstall tensorboard-plugin-wit
# !tensorboard --logdir='/content/drive/My Drive/Colab Notebooks/Hi_Im_Bot/tflearn_logs'

# save all of our data structures
import pickle
pickle.dump( {'words':words, 'classes':classes, 'train_x':train_x, 'train_y':train_y}, open( "training_data", "wb" ) )